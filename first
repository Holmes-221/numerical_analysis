分析\sum_{n=1}^{10000}\frac{1}{n}和n=1000011n的区别
先我们记计算方式\sum_{n=1}^{10000}\frac{1}{n}为方法1，\sum_{n=10000}^{1}\frac{1}{n}为方法2。
可以看到方法1得到的结果为9.787606036044348，方法2得到的结果为9.787606036044386。
虽然方法1和方法2的准确值是一样的，但在计算时两者还是出现了不同的计算结果(小数点后的第14位开始不相同)。
由于在计算机中数字存储的精度和位数有限，再进行数据计算时会截取数据的一部分存储在计算机中参与运算。
即会出现截断误差，但是这种误差是不可避免的。故若两个相差很大的数相加会忽略掉小的那个数，从而产生误差。
而若两个相差较小的数相加会有较高的精度，会减小误差，从而与真实值更接近。
我们计算：23456+0.2+0.4+0.4； 
如果按照：23456+0.2+0.4+0.4计算，会得到结果23456；
而将其重新排序计算：0.2+0.4+0.4+23456，会得到23457。
显然，23457才是我们需要的答案。 
由此可见，在做加法时应避免两个数量级相差较大的数相加(防止计算机出现大数吃掉小数的情形)。
特别地，当所求和的每项均为正数时，不正确的计算会造成结果比真实值偏小。
因此，从1加到10000的结果比从10000加到1的结果更为精确。
计算的精度与计算机的硬件性能有关。
若计算机的精度非常高，则两者的数字结果上的差距就会缩小，接近真实值。
相反，若计算机的精度较低，那么这两组数据的差异会特别大。
如果计算机能完全表示某些数(没有截断误差和舍入误差)，那么所得的结果与计算的方法没有关系。
例如：计算机能完整的表示整数，\sum_{n=1}^{10000}n和\sum_{n=10000}^{1}n的结果相同，都为50005000.0。
